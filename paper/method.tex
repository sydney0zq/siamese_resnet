% vim:ft=tex:
%
\documentclass[12pt]{article}

\title{
	method
}
\author{
	zq --- \texttt{zq@mclab}
}

\begin{document}
\maketitle

\section{2 DiffNetwork}

This section mainly describes our network framework for detecting the difference between the two similar images, including our training strategy. Then experiment results and our analysis on them will be presented afterwards.\\

\section{2.1 Data Preprocessing and Label Format}

Labeling training data is really a time-consuming and money-consuming boring work for human beings. Unfortunately, we collect few data and filter some noisy image pairs. So data augmentation need to be designed carefully. Here we use a widely used strategy. We set a jetter parameter, and then we crop or pad jetter percentage of original image size, and then we scale it to $512x512$ to fit our model. Of course, normalization never absents. But we can set our jetter bigger because our dataset have thick border. Another useful trick is we can switch image A and image B randomly, along with their corrsponding label. The second strategy boosts our performance in that our dataset have very few labels on image Bs.\\

We divide our images into $14x14$ grid cells. Each cell has a ground truth $[P(Object)][P(A|Object), P(B|Object), midx, midy, w, h]$ If one specified cell, image A and image B both exist one object to each other. Then we label $P(A|Object)=1$ and $P(B|Object)=1$, but we have only one bounding box size. We here just either one because the two objects in the rough have same bounding box shape and size.\\


\section{2.2 Model Architecture}

We treat this problem as a regression problem, like YOLO[r]. Our network structure is very simple, as we use a very small dataset which contains approximately 500 pair images in the training and validating phase,  50 pair images in the testing phase, therefore a pretrained model design is necessary. In our project we choose the famous network architecture, ResNet[r]. It is worth mentioning that our model is fully convolutional.\\

Also, the depth of ResNet is variant, such as 18, 34, or even deeper, 152. Considering the best performance of ResNet on ImageNet challenge[r] is classification and object localization. One intuitive explaination on this network is that higher level layers contains much higher semantic information, and what a classifier needs certainly is global infomation and the most significat part of given images but our task is converse. And I have made several experiments on it to verify this explaination. However, at the same time we should be aware of shallow features of our model may lead to a mAP decrease. It reasons that lower level features cannot provide good discriminative features to regressor.\\

The task our network is to find the difference of one pair images, and the two images were shootted in an almost same view angle. Our network needs to extract the lower levels information to regressor instead of higher ones. Under this core idea, we finally use a clipped ResNet18. Our model takes two images as inputs, notice they share weights, and then we use a four layers convention convolutional neural network each with ReLU unlinear activation layer to get confidence, probablity of which images as well as our bounding boxs.\\

During our exploration, we notice data argumentation really push our model to a high level, about $0.1$ in mAP. And learning scheduler also plays a center role in our model. So the optimization method may influence a lot, we should think twice before we train and test our model.\\

Like YOLO, we divide our input images into $14x14$ cells. In YOLO paper, they divide one images into $7x7$ cells. We divide our images into more cells for there are a lot of bordering conditions, while in Pascal dataset, this kind situation is not so many as ours. And we output each cell $7$ value, the first position means whether this cell exists object, we refer it as $P(Object)$; the neighboring two positions means which image has the newly added object, we can refer them as $P(A|Object), P(B|Object)$, respectively; and the last four positions stands for the bounding box regressed by the current cell, we refer them as $[midx, midy, w, h]$. $midx, midy$ are the center of our bounding box and $w, h$ are the width and height. Unlike YOLO, we notice each training instance has very few objects, so we don't actually need too many bounding box on each grid cell.\\

As a result, our network finally throws out a $7x14x14$ Tensor. We can just use formula $P(Object)\times P(A|Object)$ and $P(Object) \times P(B|Object)$ to get the probability of the bounding box in image A and image B.\\

\section{2.3 Loss Function}

Loss function controls what our model will learn, which shows the performance of our current model and the core element in optimizing our model. A well designed loss function has a decisive impact on our final result. We first try out several trial and casual loss function, but they lead to ordinary, even worse, results. So we finally imitate and modify YOLO loss function, and this performace not bad. Our final loss function is written as follows:\\

$$L(P(Object),P(A|Object),P(B|Object),x_{ i },y_{ i },w_{ i },h_{ i })=\lambda _{ coord }\sum _{ i=0 }^{ S^{ 2 } } 1_{ i }^{ obj }((x_{ i }-\hat { x } _{ i })^{ 2 }+(y_{ i }-\hat { y } _{ i })^{ 2 })+\lambda _{ coord }\sum _{ i=0 }^{ S^{ 2 } } 1_{ i }^{ obj }((w_{ i }-\hat { w } _{ i })^{ 2 }+(h_{ i }-\hat { h } _{ i })^{ 2 })+\sum _{ i=0 }^{ S^{ 2 } } 1_{ i }^{ obj }(P(Object)_{ i }-\hat { P(Object) } _{ i })^{ 2 }+\lambda _{ noobj }\sum _{ i=0 }^{ S^{ 2 } } 1_{ i }^{ obj }(P(Object)_{ i }-\hat { P(Object) } _{ i })^{ 2 }+\sum _{ i=0 }^{ S^{ 2 } } 1_{ i }^{ obj }\sum _{ c\in \{ A,\quad B\}  } (P_{ i }(c|Object)-\hat { P_{ i }(c|Object) } )^{ 2 }$$

As we can see, our loss function takes in $7x14x14$ values, we calculate each grid loss seperately. This loss function is different with YOLO, though its format is similar. Firstly, there are no so small objects like Pascal in our dataset, so we don't need square root operation to expand. Secondly, our class semantic is really novel. Our class means which image current grid bounding box belongs to, while in YOLO, it indicates what the object is, and 20 different categories in total. Thirdly, we only predict one bounding box for each grid. We think one bounding box is enough for this task and converges faster.\\


\section{2.4 Optimization}

What a optimizer does is updating all parameters in our model by current effective strategy, Stochastic Gradient Descent and Back Propgation. We also combine current fashion trick used in optimzation stage. In our routine result, our optimizer is SGD with $momentum = 0.9$. And we set our weight decay to $5e-4$, a small number. We have few training examples so we set weight decay weaker, which is a rule of thumb. As far as I observe, the learning scheduler and epoch number are the most significant factor. We use a exponent learning rate scheduler and the initial learning rate is 0.002. We decay our learning rate by factor $0.9$ every $100$ epochs. And we also tried different epochs number to squeeze the model potential.\\


\section {2.5 Metric}

In this task, we follow PASCAL VOC criterion, mAP@0.5(mean Average Precision)[r], the open and most popular metric in many challenges and datasets in detection area. \\


\section {3 Experiments}

\section {3.1 Different depth of network}

To be honest, how deep should our model be can not be calculated or analyzed precisely. We could only make several experiments and choose the reasonable and those best ones. At first, we use several different but alike models and same hyper parameters. The first one is a very tiny ResNet, the second one is deeper and the third one is the deepest model and worst performance model.\\

Conv2d 3x3 64->64
Conv2d 3x3 64->64

Conv2d 3x3 64->64
Conv2d 3x3 64->64

Conv2d 3x3 64->128
Conv2d 3x3 128->128

Conv2d 3x3 128->128
Conv2d 3x3 128->128











































\end{document}
