% vim:ft=tex:
%
\documentclass[12pt]{article}

\title{
	method
}
\author{
	zq --- \texttt{zq@mclab}
}

\begin{document}
\maketitle

\section{2 DiffNetwork}

This section mainly describes our network framework for detecting the difference between the two similar images, including our training strategy. Then experiment results and our analysis on them will be presented afterwards.\\

\section{2.1 Data Preprocessing and Label Format}


\section{2.2 Model Architecture}

We treat this problem as a regression problem, like YOLO[r]. Our network structure is very simple, as we use a very small dataset which contains approximately 500 pair images in the training and validating phase,  50 pair images in the testing phase, therefore a pretrained model design is necessary. So in our project we choose the famous network architecture, ResNet[r].\\

Also, the depth of ResNet is variant, such as 18, 34, or even deeper, 152. Considering the best performance of ResNet on ImageNet challenge[r] is classification and object localization. One intuitive explaination on this network is that higher level layers contains more higher semantic information, and what a classifier needs is certainly global infomation and the most significat part of given images while our task is converse. And I have do some experiments about it to verify this explaination.\\

The task our network is to find the difference of one pair images, and the two images were shootted in an almost same view angle. Our network needs to extract the lower levels information to regressor instead of higher ones. Under this core idea, we finally use a clipped ResNet18. Our model takes two images as inputs, notice they share the weights, and then we use a four layers convention convolutional neural network each with ReLU unlinear activation layer to get confidence, probablity of which images as well as our bounding boxs.\\

Like YOLO, we divide our input images into $14x14$ cells. In YOLO paper, they divide one images into $7x7$ cells. We divide our images into more cells for there are a lot of bordering conditions, while in Pascal dataset, this kind situation is not so many as ours. And we output each cell $7$ value, the first position means whether this cell exists object, we refer it as $P(Object)$; the neighboring two positions means which image has the newly added object, we can refer them as $P(A|Object), P(B|Object)$, respectively; and the last four positions stands for the bounding box regressed by the current cell, we refer them as $[midx, midy, w, h]$. $midx, midy$ are the center of our bounding box and $w, h$ are the width and height. Unlike YOLO, we notice each training instance has very few objects, so we don't actually need too many of boudning box on each grid cell.

As a result, our network finally throws out a $7x14x14$ Tensor. We can just use formula $P(Object)\times P(A|Object)$ and $P(Object) \times P(B|Object)$ to get the probability of the bounding box in image A and image B.













































\end{document}
